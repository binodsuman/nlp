{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts Of Speech [POS] Tagging\n",
    "\n",
    "Below code is developed to predict the Part of Speech (POS) tag for each word in a provided sentence.\n",
    "I have build a model using Hidden Markov Models which predicts the POS tags for all words.\n",
    "\n",
    "### What is a POS tag?\n",
    "\n",
    "In corpus linguistics, part-of-speech tagging (grammatical tagging or word-category disambiguation), is the process of marking up a word in a corpus as corresponding to a particular part of speech, based on both its definition and its context—i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. \n",
    "\n",
    "### Dataset\n",
    "\n",
    "File name is data.txt, attached with code.\n",
    "\n",
    "#### Dataset Description\n",
    "##### Sample Tuple\n",
    "b100-5507\n",
    "\n",
    "Mr.\tNOUN\n",
    "<br>\n",
    "Podger\tNOUN\n",
    "<br>\n",
    "had\tVERB\n",
    "<br>\n",
    "thanked\tVERB\n",
    "<br>\n",
    "him\tPRON\n",
    "<br>\n",
    "gravely\tADV\n",
    "<br>\n",
    ",\t.\n",
    "<br>\n",
    "and\tCONJ\n",
    "<br>\n",
    "now\tADV\n",
    "<br>\n",
    "he\tPRON\n",
    "<br>\n",
    "made\tVERB\n",
    "<br>\n",
    "use\tNOUN\n",
    "<br>\n",
    "of\tADP\n",
    "<br>\n",
    "the\tDET\n",
    "<br>\n",
    "advice\tNOUN\n",
    "<br>\n",
    ".\t.\n",
    "<br>\n",
    "##### Explanation\n",
    "The first token \"b100-5507\" is just a key and acts like an identifier to indicate the beginning of a sentence.\n",
    "<br>\n",
    "The other tokens have a (Word, POS Tag) pairing.\n",
    "\n",
    "__List of POS Tags are:__\n",
    ".\n",
    "<br>\n",
    "ADJ\n",
    "<br>\n",
    "ADP\n",
    "<br>\n",
    "ADV\n",
    "<br>\n",
    "CONJ\n",
    "<br>\n",
    "DET\n",
    "<br>\n",
    "NOUN\n",
    "<br>\n",
    "NUM\n",
    "<br>\n",
    "PRON\n",
    "<br>\n",
    "PRT\n",
    "<br>\n",
    "VERB\n",
    "<br>\n",
    "X\n",
    "\n",
    "__Note__\n",
    "<br>\n",
    "__.__ is used to indicate special characters such as '.', ','\n",
    "<br>\n",
    "__X__ is used to indicate vocab not part of Enlish Language mostly.\n",
    "Others are Standard POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required libraries\n",
    "\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import defaultdict, namedtuple, OrderedDict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all data from given dataset.\n",
    "# All method and class inisde this block, for data reading from both file data and tags files.\n",
    "# These are make our data processing (reading, parsing) and data spliting work easy.\n",
    "\n",
    "Sentence = namedtuple(\"Sentence\", \"words tags\")\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        sentence_lines = [l.split(\"\\n\") for l in f.read().split(\"\\n\\n\")]\n",
    "    return OrderedDict(((s[0], Sentence(*zip(*[l.strip().split(\"\\t\")\n",
    "                        for l in s[1:]]))) for s in sentence_lines if s[0]))\n",
    "\n",
    "def read_tags(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        tags = f.read().split(\"\\n\")\n",
    "    return frozenset(tags)\n",
    "\n",
    "class Subset(namedtuple(\"BaseSet\", \"sentences keys vocab X tagset Y N stream\")):\n",
    "    def __new__(cls, sentences, keys):\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        wordset = frozenset(chain(*word_sequences))\n",
    "        tagset = frozenset(chain(*tag_sequences))\n",
    "        N = sum(1 for _ in chain(*(sentences[k].words for k in keys)))\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, {k: sentences[k] for k in keys}, keys, wordset, word_sequences,\n",
    "                               tagset, tag_sequences, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())\n",
    "\n",
    "class Dataset(namedtuple(\"_Dataset\", \"sentences keys vocab X tagset Y training_tags_set testing_tags_set N stream\")):\n",
    "    def __new__(cls, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "        sentences = read_data(datafile)\n",
    "        keys = tuple(sentences.keys())\n",
    "        wordset = frozenset(chain(*[s.words for s in sentences.values()]))\n",
    "        word_sequences = tuple([sentences[k].words for k in keys])\n",
    "        tag_sequences = tuple([sentences[k].tags for k in keys])\n",
    "        N = sum(1 for _ in chain(*(s.words for s in sentences.values())))\n",
    "        \n",
    "        # split data into train/test sets\n",
    "        _keys = list(keys)\n",
    "        if seed is not None: random.seed(seed)\n",
    "        random.shuffle(_keys)\n",
    "        split = int(train_test_split * len(_keys))\n",
    "        training_tags_data = Subset(sentences, _keys[:split])\n",
    "        testing_tags_data = Subset(sentences, _keys[split:])\n",
    "        stream = tuple(zip(chain(*word_sequences), chain(*tag_sequences)))\n",
    "        return super().__new__(cls, dict(sentences), keys, wordset, word_sequences, tagset,\n",
    "                               tag_sequences, training_tags_data, testing_tags_data, N, stream.__iter__)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentences.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(words=('Whenever', 'artists', ',', 'indeed', ',', 'turned', 'to', 'actual', 'representations', 'or', 'molded', 'three-dimensional', 'figures', ',', 'which', 'were', 'rare', 'down', 'to', '800', 'B.C.', ',', 'they', 'tended', 'to', 'reflect', 'reality', '(', 'see', 'Plate', '6a', ',', '9b', ')', ';', ';'), tags=('ADV', 'NOUN', '.', 'ADV', '.', 'VERB', 'ADP', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'ADJ', 'NOUN', '.', 'DET', 'VERB', 'ADJ', 'PRT', 'ADP', 'NUM', 'NOUN', '.', 'PRON', 'VERB', 'PRT', 'VERB', 'NOUN', '.', 'VERB', 'NOUN', 'NUM', '.', 'NUM', '.', '.', '.'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All required Pre-process data \n",
    "# tags_data will store data and tage after parsing with train ration 80%.\n",
    "tagset = {'PRON', 'ADJ', 'NUM', 'DET', 'NOUN', 'VERB', 'X', 'CONJ', 'ADV', 'PRT', 'ADP', '.'}\n",
    "tags_data = Dataset(tagset, \"data.txt\", train_test_split=0.8)\n",
    "#tags_data.training_tags_set[0]\n",
    "#Below is just to show one training record.\n",
    "np.array(tags_data.training_tags_set)[0][1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mr.', 'Podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.')\n",
      "('NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.')\n"
     ]
    }
   ],
   "source": [
    "# See, how data appear after processing. Just checking with first record of dataset.\n",
    "# X contains words and Y contains respective tags.\n",
    "\n",
    "print( tags_data.X[0])\n",
    "print( tags_data.Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag to most frequenlty assigned to the word in sequence\n",
    "# To find most freqent class\n",
    "\n",
    "def pair_counts(tags, words):\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        d[tag][word] += 1\n",
    "    return d\n",
    "\n",
    "# input sequence where each unknown word is replaced by the literal string value 'nan'\n",
    "def replace_unknown(sequence):\n",
    "    return [w if w in tags_data.training_tags_set.vocab else 'nan' for w in sequence]\n",
    "\n",
    "# Taking care for one dimension observations.\n",
    "def simplify_decoding(X, model):\n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]\n",
    "\n",
    "# Returned Dictionary keyed to each unique value in the input sequence list that\n",
    "# counts the number of occurrences of the value in the sequences list.\n",
    "\n",
    "import itertools\n",
    "def unigram_counts(sequences):\n",
    "    sequences = itertools.chain.from_iterable(sequences)\n",
    "    dictionary = {}\n",
    "    \n",
    "    for i_seq in sequences:\n",
    "        if i_seq in dictionary.keys():\n",
    "            dictionary[i_seq] += 1\n",
    "        else:\n",
    "            dictionary[i_seq] = 1\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "tag_unigrams = unigram_counts(tags_data.training_tags_set.Y)\n",
    "\n",
    "\n",
    "# Returned Dictionary keyed to each unique value in the input sequence list that\n",
    "# counts the number of occurrences of the value in the sequences list for value at the staring of sequence.\n",
    "\n",
    "def starting_counts(sequences):\n",
    "    start_tags = []\n",
    "    for i_seq in sequences:\n",
    "        start_tags.append(i_seq[0])\n",
    "    \n",
    "    dictionary = {}\n",
    "    for i_tag in start_tags:\n",
    "        if i_tag in dictionary.keys():\n",
    "            dictionary[i_tag] += 1\n",
    "        else:\n",
    "            dictionary[i_tag] = 1\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "tag_starts = starting_counts(tags_data.training_tags_set.Y)\n",
    "\n",
    "# Returned Dictionary keyed to each unique value in the input sequence list that\n",
    "# counts the number of occurrences of the value in the sequences list for value at the end of sequence.\n",
    "\n",
    "def ending_counts(sequences):\n",
    "    start_tags = []\n",
    "    for i_seq in sequences:\n",
    "        start_tags.append(i_seq[-1])\n",
    "    \n",
    "    dictionary = {}\n",
    "    for i_tag in start_tags:\n",
    "        if i_tag in dictionary.keys():\n",
    "            dictionary[i_tag] += 1\n",
    "        else:\n",
    "            dictionary[i_tag] = 1\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "tag_ends = ending_counts(tags_data.training_tags_set.Y)\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Returned unique pair of value and number of occurance of pair in the sequence list.\n",
    "def bigram_counts(sequences):\n",
    "    dictionary = {}\n",
    "    bigrams = list(nltk.bigrams(tags))\n",
    "    for i_bigram in bigrams:\n",
    "        if i_bigram in dictionary.keys():\n",
    "            dictionary[i_bigram] += 1\n",
    "        else:\n",
    "            dictionary[i_bigram] = 1\n",
    "            \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "tags = []\n",
    "for i in tags_data.training_tags_set.Y:\n",
    "    for j in i:\n",
    "        tags.append(j)\n",
    "        \n",
    "tag_bigrams = bigram_counts(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes or States:  14\n",
      "Number of Edges:  168\n"
     ]
    }
   ],
   "source": [
    "#HMM Model is contructing here\n",
    "\n",
    "# Creating base HMM base model.\n",
    "basic_hmm_model = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "# fetch tags and words from data stream.\n",
    "tags = [tag for _, tag in tags_data.stream()]\n",
    "words = [word for word, _ in tags_data.stream()]\n",
    "\n",
    "# Finding emission counts\n",
    "emission_counts = pair_counts(tags, words)\n",
    "\n",
    "states = {}\n",
    "\n",
    "\n",
    "for i_tag in tags_data.tagset:\n",
    "    \n",
    "    emission_probabilities = dict()\n",
    "    for i_word, i_occurance in emission_counts[i_tag].items(): \n",
    "        emission_probabilities[i_word] = i_occurance / tag_unigrams[i_tag] \n",
    "    \n",
    "    tag_distribution = DiscreteDistribution(emission_probabilities) \n",
    "    state = State(tag_distribution, name=i_tag)\n",
    "    states[i_tag] = state\n",
    "    \n",
    "    # Assigning issue state to our model\n",
    "    basic_hmm_model.add_state(state)\n",
    "\n",
    "\n",
    "for tag in tags_data.tagset:\n",
    "    state = states[tag]\n",
    "    \n",
    "    # Calculate the start tag probability\n",
    "    start_probability = tag_starts[tag] / sum(tag_starts.values())\n",
    "    \n",
    "    # Probability - states\n",
    "    basic_hmm_model.add_transition(basic_hmm_model.start, state, start_probability)\n",
    "    \n",
    "    # End tag probability\n",
    "    end_probability = tag_ends[tag] / sum(tag_ends.values())\n",
    "    \n",
    "    # Probability in between states\n",
    "    basic_hmm_model.add_transition(state, basic_hmm_model.end, end_probability)\n",
    "\n",
    "\n",
    "\n",
    "for tag_1 in tags_data.tagset:\n",
    "    \n",
    "    state_1 = states[tag_1]\n",
    "    \n",
    "    # Initialze the sum of probabilities to 0\n",
    "    sum_of_probabilities = 0\n",
    "\n",
    "\n",
    "    for tag_2 in tags_data.tagset:\n",
    "        state_2 = states[tag_2]\n",
    "        bigram = (tag_1, tag_2)\n",
    "        \n",
    "        # Transition probability\n",
    "        transition_probability = tag_bigrams[bigram] / tag_unigrams[tag_1]\n",
    "        \n",
    "        # Transition probability to sum_of_probabilities\n",
    "        sum_of_probabilities += transition_probability\n",
    "        \n",
    "        # Transition to our model\n",
    "        basic_hmm_model.add_transition(state_1, state_2, transition_probability)\n",
    "\n",
    "\n",
    "\n",
    "basic_hmm_model.bake()\n",
    "print(\"Nodes or States: \", basic_hmm_model.node_count())\n",
    "print(\"Number of Edges: \", basic_hmm_model.edge_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äccuracy basic hmm model: 97.53%\n",
      "Testing accuracy basic hmm model: 96.16%\n"
     ]
    }
   ],
   "source": [
    "# Here is the Model Accuracy Evaluation\n",
    "# To evaluate our model\n",
    "\n",
    "def accuracy(X, Y, model):\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions\n",
    "\n",
    "hmm_training_acc = accuracy(tags_data.training_tags_set.X, tags_data.training_tags_set.Y, basic_hmm_model)\n",
    "print(\"Äccuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(tags_data.testing_tags_set.X, tags_data.testing_tags_set.Y, basic_hmm_model)\n",
    "print(\"Testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b100-935\n",
      "Predicted Tags:\n",
      "-----------------\n",
      "['CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'PRT', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.']\n",
      "\n",
      "Actual TAgs:\n",
      "--------------\n",
      "('CONJ', 'PRT', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'ADP', 'ADV', 'ADV', 'DET', 'NOUN', 'VERB', 'VERB', '.', 'CONJ', 'DET', 'NOUN', 'PRON', 'VERB', 'VERB', '.')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "\n",
    "key = \"b100-935\"\n",
    "print(format(key))\n",
    "print(\"Predicted Tags:\\n-----------------\")\n",
    "print(simplify_decoding(tags_data.sentences[key].words, basic_hmm_model))\n",
    "print()\n",
    "print(\"Actual TAgs:\\n--------------\")\n",
    "print(tags_data.sentences[key].tags)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binod Suman Academy at YouTube"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
