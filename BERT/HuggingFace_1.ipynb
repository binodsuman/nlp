{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (4.27.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (5.1.2)\n",
      "Requirement already satisfied: requests in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (0.23)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (4.54.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from importlib-metadata->transformers) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata->transformers) (7.2.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/binod/Documents/software/anaconda/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/binod/Documents/software/anaconda/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# First, make sure you have the Transformers library installed:\n",
    "\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae1a75e2b2046988a56fd54987c48c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading pytorch_model.bin'), FloatProgress(value=0.0, max=440473133.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Next, we'll import the necessary modules and load a pre-trained BERT model:\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use the model to classify some text. \n",
    "# First, we'll tokenize the text using the BERT tokenizer, \n",
    "# which converts the text into a sequence of tokens that can be fed into the model:\n",
    "\n",
    "# Tokenize the text\n",
    "text = \"This is a test sentence\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "#The return_tensors='pt' argument tells the tokenizer to return the input as PyTorch tensors, \n",
    "# which is the format that the BERT model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231, 6251,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use the model to classify the text. We'll pass the tokenized input to the model's forward method, \n",
    "# which will return a tuple containing the model's output:\n",
    "\n",
    "# Classify the text\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2175, -0.3369]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logits variable contains the model's predictions for each class. \n",
    "# To get the predicted class, we can use the argmax method:\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = logits.argmax().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name Entity Recognition (NER) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b337bffb16f34796ac7289799747504a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)okenizer_config.json'), FloatProgress(value=0.0, max=59.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a35b195781f456492d1e9a2bec52189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)lve/main/config.json'), FloatProgress(value=0.0, max=829.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d828287b534e7390d96aa30ab90e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)solve/main/vocab.txt'), FloatProgress(value=0.0, max=213450.0), HTML…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b011d885a14872b08a28340bbcf263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)in/added_tokens.json'), FloatProgress(value=0.0, max=2.0), HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e389ba24c1834f5da876cb00a9993929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)cial_tokens_map.json'), FloatProgress(value=0.0, max=112.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5530f5472a3e487a8d52736792a93759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading pytorch_model.bin'), FloatProgress(value=0.0, max=433316646.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Load the pre-trained BERT tokenizer and model for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll tokenize some input text using the BERT tokenizer, and pass it to the model for prediction:\n",
    "\n",
    "# Tokenize the input text\n",
    "text = \"John lives in New York City\"\n",
    "tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "# Get the predicted NER labels for the tokens\n",
    "outputs = model(tokens)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "labels = [model.config.id2label[label_id] for label_id in predictions[0].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd01d6a82a2b4bfab9578904362ffc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)okenizer_config.json'), FloatProgress(value=0.0, max=48.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f5c4de6cac4e6db6513a98c5f34c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)lve/main/config.json'), FloatProgress(value=0.0, max=511.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a068f373740f4ac998f3a7722736be73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)solve/main/vocab.txt'), FloatProgress(value=0.0, max=231508.0), HTML…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19062d0b941e4623a04354cbb83f295f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)cial_tokens_map.json'), FloatProgress(value=0.0, max=112.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0decfd459331485eb32093d443c73bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading pytorch_model.bin'), FloatProgress(value=0.0, max=437985387.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First, we'll load a pre-trained BERT model for sentiment analysis:\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT tokenizer and model for sentiment analysis\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll tokenize some input text using the BERT tokenizer, and pass it to the model for prediction:\n",
    "\n",
    "# Tokenize the input text\n",
    "text = \"This movie was absolutely amazing! The acting, direction, and visuals were all incredible but songs were very bad.\"\n",
    "tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "# Get the predicted sentiment label for the tokens\n",
    "outputs = model(tokens)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "label = \"positive\" if predictions[0].item() == 1 else \"negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT model for question answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "# Define the question and context\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"France is a country in Europe. Its capital is Paris.\"\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Perform the question-answering task\n",
    "answer_start_scores, answer_end_scores = model(**inputs).values()\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "# Print the answer\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
